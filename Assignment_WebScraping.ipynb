{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb200055-53df-4dc0-bc9e-3e61778b5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1\n",
    "Web Scraping:\n",
    "It is the automated process of extracting information from websites.\n",
    "It involves using software (bots or crawlers) to fetch web pages and pull out specific data from their HTML structure.\n",
    "    \n",
    "# Why it's used:\n",
    "It allows for the efficient collection of large amounts of data from the web that would be impractical or too time-consuming to gather manually. \n",
    "This collected data can then be analyzed, compared, or used for various applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71b46c-243d-44a3-9d1f-e7a6bbb86b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "Several methods can be used for web scraping, ranging in complexity:\n",
    "\n",
    "#Manual Copy-Pasting: \n",
    "Physically copying data from websites and pasting it into a file. \n",
    "This is only feasible for very small amounts of data.\n",
    "\n",
    "#Using Web Scraping Software/Tools:\n",
    "Many browser extensions or desktop applications (e.g., ParseHub, Octoparse) provide graphical interfaces to select data for extraction without writing code.\n",
    "Programming with Libraries: This is the most flexible method. \n",
    "It involves writing code, typically in languages like Python:\n",
    "                                                                                                                       \n",
    "#Making HTTP Requests: \n",
    "Using libraries (like Python's requests) to download the raw HTML content of web pages.\n",
    "HTML Parsing: Using libraries (like Python's Beautiful Soup, lxml, or Scrapy) to parse the downloaded HTML, navigate its structure, and extract the desired data elements (text, links, image URLs, etc.).\n",
    "Browser Automation: Using tools (like Selenium or Playwright) to control a real web browser programmatically. \n",
    "This is necessary for websites that load data dynamically using JavaScript after the initial page load.\n",
    "Using Website APIs: The best method, when available. \n",
    "Many websites offer an Application Programming Interface (API) that provides direct access to their data in a structured format (like JSON), eliminating the need to parse HTML. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c1b7d-eb14-4598-b51a-a37f38b84900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "Beautiful Soup is a Python library designed for parsing HTML and XML documents.\n",
    "It takes raw HTML/XML text and creates a structured representation (a parse tree) that is easy to navigate and search.\n",
    "\n",
    "#It is used for:\n",
    "\n",
    "Handles Messy HTML: It's robust and can often parse poorly formatted or invalid HTML that might cause other parsers to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1634f1e-e6b0-4374-9943-fe147bd5c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4\n",
    "Flask is a lightweight web framework for Python. \n",
    "While web scraping itself focuses on data extraction, Flask would likely be used in a web scraping project for one or more of the following reasons:\n",
    "\n",
    "#Creating a User Interface:\n",
    "    To build a web page where a user can input a URL or parameters, trigger the scraping process, and see the results.\n",
    "#Building an API: \n",
    "    To create an API endpoint. Other applications could then send requests to this API to start a scraping job or retrieve the scraped data programmatically.\n",
    "#Displaying Scraped Data: \n",
    "    After the data is scraped and likely stored (e.g., in a database or file), Flask can be used to create web pages that present this data in a readable format, perhaps with tables, charts, or search functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70f60e-f62e-402c-8c00-9f38494119d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "EC2/Lambda: \n",
    "To run the scraping code (compute).\n",
    "\n",
    "S3:\n",
    "To store the extracted data (storage).\n",
    "\n",
    "RDS/DynamoDB: \n",
    "To store data in a structured database.\n",
    "    \n",
    "CloudWatch/EventBridge:\n",
    "To schedule scraping tasks and monitor logs.\n",
    "\n",
    "API Gateway:\n",
    "To create an API interface for the scraper.\n",
    "\n",
    "IAM:\n",
    "To manage security permissions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
